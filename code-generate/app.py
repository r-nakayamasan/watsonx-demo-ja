print("ã‚¤ãƒ³ãƒãƒ¼ãƒˆé–‹å§‹")
from typing import Iterator

import os

import gradio as gr
import torch

from threading import Thread

from genai.model import Credentials, Model #ã¾ãšå…ˆã«genaiã‚’importã€æ¬¡ã«ibm-generative-aiã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

from genai.schemas import GenerateParams

from genai.schemas import TokenParams

from queue import Queue

# å¤‰æ•°

DEFAULT_SYSTEM_PROMPT = """ä»¥ä¸‹ã¯ã€ã•ã¾ã–ã¾ãªäººã€…ã¨AIã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã®ä¸€é€£ã®å¯¾è©±ã§ã‚ã‚‹ã€‚ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¯ã€è¦ªåˆ‡ã§ã€ä¸å¯§ã§ã€æ­£ç›´ã§ã€æ´—ç·´ã•ã‚Œã¦ã„ã¦ã€æ„Ÿæƒ…çš„ã§ã€è¬™è™šã ãŒçŸ¥è­˜è±Šå¯Œã§ã‚ã‚ã†ã¨ã™ã‚‹ã€‚ã“ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¯ã€ã‚³ãƒ¼ãƒ‰ã®è³ªå•ã‚’å–œã‚“ã§æ‰‹ä¼ã„ã€ä½•ãŒå¿…è¦ã‹ã‚’æ­£ç¢ºã«ç†è§£ã™ã‚‹ãŸã‚ã«æœ€å–„ã‚’å°½ãã—ã¾ã™ã€‚ã¾ãŸã€å½ã®æƒ…å ±ã‚„èª¤è§£ã‚’æ‹›ãã‚ˆã†ãªæƒ…å ±ã‚’ä¸ãˆãªã„ã‚ˆã†ã«ã—ã€æ­£ã—ã„ç­”ãˆãŒå®Œå…¨ã«ã‚ã‹ã‚‰ãªã„ã¨ãã«ã¯æ³¨æ„ã‚’ä¿ƒã—ã¾ã™ã€‚ã¨ã¯ã„ãˆã€ã“ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¯å®Ÿç”¨çš„ã§ã€æœ¬å½“ã«ãƒ™ã‚¹ãƒˆã‚’å°½ãã—ã¦ãã‚Œã‚‹ã€‚

------

Japaneseï¼š<æŒ‡ç¤ºæ–‡>
Assistantï¼š```<å›ç­”>```"""

MAX_MAX_NEW_TOKENS = 2048
DEFAULT_MAX_NEW_TOKENS = 1024
MAX_INPUT_TOKEN_LENGTH = 4000

# ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
llm_id = 'bigcode/starcoder'

#LLMã®æº–å‚™
# APIã®keyã‚’æŒ¿å…¥
# api_key = input("ã‚ãªãŸã®BAM APIã‚’å…¥åŠ›ã—ã¦ä¸‹ã•ã„")
api_key = os.environ.get("API_KEY")
if not api_key:
    raise ValueError("API_KEY environment variable is not set!")

api_endpoint = "https://bam-api.res.ibm.com/v1/"
creds = Credentials(api_key=api_key, api_endpoint=api_endpoint)

# # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
tokenizer = Model(llm_id, params=TokenParams, credentials=creds)

#ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã®ã‚¯ãƒªã‚¢ãƒ¼
def clear_and_save_textbox(message: str) -> tuple[str, str]:
    return '', message

#ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚¤ãƒ³ãƒ—ãƒƒãƒˆ
def display_input(message: str,
                  history: list[tuple[str, str]]) -> list[tuple[str, str]]:
    history.append((message, ''))
    return history

def delete_prev_fn(
        history: list[tuple[str, str]]) -> tuple[list[tuple[str, str]], str]:
    try:
        message, _ = history.pop()
    except IndexError:
        message = ''
    return history, message or ''

def get_prompt(message: str, chat_history: list[tuple[str, str]],
               system_prompt: str) -> str:

#     texts = [f'''<s>[INST] <<SYS>>
# {system_prompt}
# <</SYS>>

# ã‚ãªãŸã¯ã€Œwatsonx.aiã€ã¨ã„ã†åå‰ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨æ˜ã‚‹ãä¼šè©±ã—ã¾ã™ã€‚æ—¥æœ¬èªã§ä¼šè©±ã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚
# ''']

    texts = [f"""{system_prompt}

-----"""]
    
    # The first user input is _not_ stripped(strip=å‰å¾Œã®ç©ºç™½ãŒå‰Šé™¤ã•ã‚Œã‚‹)
    do_strip = False
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’textesã«è¿½åŠ 
    for user_input, response in chat_history:
        user_input = user_input.strip() if do_strip else user_input
        do_strip = True
#         texts.append(f'ãƒ¦ãƒ¼ã‚¶ãƒ¼: {user_input} [/INST] ã‚ãªãŸ: {response.strip()} </s><s>[INST] ')
        texts.append(f"""

Japaneseï¼š{user_input}
Assistantï¼š{response.strip()}""")
        
    #2å›ç›®ä»¥é™
    #messageã¯ä»Šå›ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    if message is not None:
        message = message.strip() if do_strip else message
    else:
        message = ""

#     texts.append(f"{message} [/INST] ã‚ãªãŸ: ")
    texts.append(f"""

Japaneseï¼š{message} 
Assistantï¼š
""")

    return ''.join(texts)


def get_input_token_length(message: str, chat_history: list[tuple[str, str]], system_prompt: str) -> int:
    prompt = get_prompt(message, chat_history, system_prompt)
    tokenized_response = tokenizer.tokenize([prompt], return_tokens=True)
    input_ids = tokenized_response[0].token_count
    return input_ids

def run(message: str,
        chat_history: list[tuple[str, str]],
        system_prompt: str,
        max_new_tokens: int,
        temperature: float,
        top_p: float,
        top_k: int) -> Iterator[str]:
    
    prompt = get_prompt(message, chat_history, system_prompt)
    q = Queue()

    params = GenerateParams(
        decoding_method="sample",
        stream=True,
        max_new_tokens=max_new_tokens,
        top_p=top_p,
        top_k=top_k,
        temperature=temperature,
        repetition_penalty=1.0,
#         random_seed=1,
#         beam_width=1,
        stop_sequences=["-----"]
    )
    
    llm = Model(model=llm_id, params=params, credentials=creds)
    
    print("ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡ºåŠ›ã—ã¾ã™")
    print(params)
    print(prompt)
    
    def threaded_generate(q: Queue, prompt: str):
        for chunk in llm.generate_stream([prompt]):
            q.put(chunk)
        q.put(None)
    
    t = Thread(target=threaded_generate, args=(q, prompt))
    t.start()

    outputs = []
    
    while True:
        chunk = q.get()
        if chunk is None:
            break
        if chunk.generated_text is not None:
            outputs.append(chunk.generated_text)
            print(outputs)
        yield ''.join(outputs)
        

def generate(
    message: str,
    history_with_input: list[tuple[str, str]],
    system_prompt: str,
    max_new_tokens: int,
    temperature: float,
    top_p: float,
    top_k: int,
) -> Iterator[list[tuple[str, str]]]:
    if max_new_tokens > MAX_MAX_NEW_TOKENS:
        raise ValueError
    
    #å…¥åŠ›å—ã‘å–ã‚Š
    history = history_with_input[:-1]
    
    #å‡ºåŠ›
    generator = run(message, history, system_prompt, max_new_tokens, temperature, top_p, top_k)
    try:
        first_response = next(generator)
        yield history + [(message, first_response)]
    except StopIteration:
        yield history + [(message, '')]
    for response in generator:
        yield history + [(message, response)]
        

def process_example(message: str) -> tuple[str, list[tuple[str, str]]]:
    generator = generate(message, [], DEFAULT_SYSTEM_PROMPT, 1024, 1, 0.95, 50)
    for x in generator:
        pass
    return '', x

def check_input_token_length(message: str, chat_history: list[tuple[str, str]], system_prompt: str) -> None:
    input_token_length = get_input_token_length(message, chat_history, system_prompt)
    if input_token_length > MAX_INPUT_TOKEN_LENGTH:
        raise gr.Error(f'The accumulated input is too long ({input_token_length} > {MAX_INPUT_TOKEN_LENGTH}). Clear your chat history and try again.')
    
with gr.Blocks(css='style.css') as demo:

    with gr.Group():
        chatbot = gr.Chatbot(label='Chatbot')
        with gr.Row():
            textbox = gr.Textbox(
                container=False,
                show_label=False,
                placeholder='ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ã­ï¼',
                scale=10,
            )
            submit_button = gr.Button('é€ä¿¡âœˆï¸',
                                      variant='primary',
                                      scale=1,
                                      min_width=0)
    with gr.Row():
        retry_button = gr.Button('ğŸ”„  ã‚‚ã†ä¸€åº¦èã„ã¦ã¿ã‚‹', variant='secondary')
        undo_button = gr.Button('â†©ï¸ å…ƒã«æˆ»ã™', variant='secondary')
        clear_button = gr.Button('ğŸ—‘ï¸  ä¼šè©±ã‚’å‰Šé™¤ã™ã‚‹', variant='secondary')

    saved_input = gr.State()

    with gr.Accordion(label='è©³ç´°è¨­å®š', open=False):
        system_prompt = gr.Textbox(label='System prompt',
                                   value=DEFAULT_SYSTEM_PROMPT,
                                   lines=6)
        max_new_tokens = gr.Slider(
            label='Max new tokens',
            minimum=1,
            maximum=MAX_MAX_NEW_TOKENS,
            step=1,
            value=DEFAULT_MAX_NEW_TOKENS,
        )
        temperature = gr.Slider(
            label='Temperature',
            minimum=0.1,
            maximum=4.0,
            step=0.1,
            value=0.3,
        )
        top_p = gr.Slider(
            label='Top-p (nucleus sampling)',
            minimum=0.05,
            maximum=1.0,
            step=0.05,
            value=0.95,
        )
        top_k = gr.Slider(
            label='Top-k',
            minimum=1,
            maximum=1000,
            step=1,
            value=50,
        )


    textbox.submit(
        fn=clear_and_save_textbox,
        inputs=textbox,
        outputs=[textbox, saved_input],
        api_name=False,
        queue=False,
    ).then(
        fn=display_input,
        inputs=[saved_input, chatbot],
        outputs=chatbot,
        api_name=False,
        queue=False,
    ).then(
        fn=check_input_token_length,
        inputs=[saved_input, chatbot, system_prompt],
        api_name=False,
        queue=False,
    ).success(
        fn=generate,
        inputs=[
            saved_input,
            chatbot,
            system_prompt,
            max_new_tokens,
            temperature,
            top_p,
            top_k,
        ],
        outputs=chatbot,
        api_name=False,
    )

    button_event_preprocess = submit_button.click(
        fn=clear_and_save_textbox,
        inputs=textbox,
        outputs=[textbox, saved_input],
        api_name=False,
        queue=False,
    ).then(
        fn=display_input,
        inputs=[saved_input, chatbot],
        outputs=chatbot,
        api_name=False,
        queue=False,
    ).then(
        fn=check_input_token_length,
        inputs=[saved_input, chatbot, system_prompt],
        api_name=False,
        queue=False,
    ).success(
        fn=generate,
        inputs=[
            saved_input,
            chatbot,
            system_prompt,
            max_new_tokens,
            temperature,
            top_p,
            top_k,
        ],
        outputs=chatbot,
        api_name=False,
    )

    retry_button.click(
        fn=delete_prev_fn,
        inputs=chatbot,
        outputs=[chatbot, saved_input],
        api_name=False,
        queue=False,
    ).then(
        fn=display_input,
        inputs=[saved_input, chatbot],
        outputs=chatbot,
        api_name=False,
        queue=False,
    ).then(
        fn=generate,
        inputs=[
            saved_input,
            chatbot,
            system_prompt,
            max_new_tokens,
            temperature,
            top_p,
            top_k,
        ],
        outputs=chatbot,
        api_name=False,
    )

    undo_button.click(
        fn=delete_prev_fn,
        inputs=chatbot,
        outputs=[chatbot, saved_input],
        api_name=False,
        queue=False,
    ).then(
        fn=lambda x: x,
        inputs=[saved_input],
        outputs=textbox,
        api_name=False,
        queue=False,
    )

    clear_button.click(
        fn=lambda: ([], ''),
        outputs=[chatbot, saved_input],
        queue=False,
        api_name=False,
    )

print("å‡¦ç†å®Œäº†")

demo.queue(max_size=20).launch(server_name="0.0.0.0", server_port=8000)
