{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13eff645",
   "metadata": {},
   "source": [
    "# Q&A\n",
    "使用するpdfファイルはpdfsの下に格納"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015f904a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/makyua452144/anaconda3/envs/watsonx_demo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/makyua452144/anaconda3/envs/watsonx_demo/lib/python3.11/site-packages/genai/schemas/__init__.py:10: DeprecationWarning: \u001b[33;20m\n",
      "The class ModelType is being deprecated.\n",
      "Please replace any reference to ModelType by its model id string equivalent.\n",
      "Example :\n",
      "  ModelType.FLAN_T5 becomes \"google/flan-t5-xxl\"\u001b[0m\n",
      "\n",
      "  from genai.schemas.models import ModelType\n"
     ]
    }
   ],
   "source": [
    "#インポートするファイルまとめ\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import gradio as gr\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from genai.model import Credentials, Model\n",
    "from genai.schemas import GenerateParams, ModelType\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb861f4",
   "metadata": {},
   "source": [
    "# 調整できるパラメーター\n",
    "- chunk_liit: チャンクに含めることができる最大文字数\n",
    "- chunk_overlap: AIが完全に文脈を見失わないように、chunkにまたがる文字数\n",
    "- min_new_tokens: 生成された応答のトークンの最小数\n",
    "- max_new_tokens: 生成された応答のトークンの最大数\n",
    "- repetition_penalty: 生成したテキストに基づいて、新しいトークンに課すペナルティ\n",
    "- search_kwargs: k(返されるドキュメントの数)、score_threshold(リトリーバーから返されるドキュメントの最小関連数)、fetch_k(MMRに渡すドキュメントの数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43d04923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#設定する箇所\n",
    "chunk_limit = 700\n",
    "chunk_overlap = 10\n",
    "min_new_tokens=3\n",
    "max_new_tokens=1500\n",
    "repetition_penalty=1.1\n",
    "search_kwargs={'k': 5, 'fetch_k': 50, 'score_threshold': 0.8}\n",
    "\n",
    "# APIのkeyを挿入\n",
    "api_key = \"pak-aiAAErKRTfrLuK3Gl0p7-HOvMnJsU4UwmFf9wLJCaiA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d85822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#変更しない\n",
    "\n",
    "pdf_folder_path = './pdfs'\n",
    "# print(os.listdir(pdf_folder_path))\n",
    "\n",
    "#LLMの準備\n",
    "api_endpoint = \"https://bam-api.res.ibm.com/v1/\"\n",
    "\n",
    "#ベクターストアの場所\n",
    "vec = \"article_\" +  str(chunk_limit) + \"_\" + str(chunk_overlap)\n",
    "persist_directory=\"vec/\" + vec\n",
    "\n",
    "creds = Credentials(api_key=api_key, api_endpoint=api_endpoint)\n",
    "\n",
    "# PDFのローディング\n",
    "loaders = [UnstructuredPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)] #ローダー定義\n",
    "documents = [docu.load() for docu in loaders]\n",
    "new_documents = \"\"\n",
    "for i in range(len(documents)):\n",
    "    new_documents += documents[i][0].page_content\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "\n",
    "if vec in os.listdir(\"vec/\"):\n",
    "    #再利用\n",
    "    index = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "else:\n",
    "    \n",
    "    #チャンクの分割\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_limit, chunk_overlap=chunk_overlap)\n",
    "    pdf_text = text_splitter.split_text(new_documents)\n",
    "    \n",
    "    # ドキュメントの読みこみ\n",
    "    index = Chroma.from_texts(pdf_text, embeddings, persist_directory=persist_directory)\n",
    "    \n",
    "params_qa = GenerateParams(\n",
    "    decoding_method=\"greedy\",\n",
    "    min_new_tokens=min_new_tokens,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    stream=False,\n",
    "    repetition_penalty=repetition_penalty,\n",
    ").dict() \n",
    "\n",
    "llm_qa = LangChainInterface(model='meta-llama/llama-2-70b-chat', credentials=creds, params=params_qa)\n",
    "\n",
    "#リトリーバーの設定(ユーザーから質問を受け取り、ベクターストアに投げかけ、関連コードを返してくれるところ)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm_qa, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=index.as_retriever(search_kwargs=search_kwargs), \n",
    "                                    input_key=\"question\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23496e65",
   "metadata": {},
   "source": [
    "new_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b559bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa(pdf_text, question):\n",
    "    template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "----------------\n",
    "{ground_text}\n",
    "----------------\n",
    "                \n",
    "----------------\n",
    "Question: {question}\n",
    "----------------\n",
    "\n",
    "You must answer in Japanese.\n",
    "\n",
    "Helpful answer in Japanese:\"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=template, \n",
    "        input_variables=[\"ground_text\", \"question\"]\n",
    "    )\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm_qa, verbose=True)\n",
    "    answer = llm_chain.predict(ground_text=pdf_text, question=question)\n",
    "     \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b94b4101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_qa(doc, q, ans):\n",
    "    template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "----------------\n",
    "{ground_text}\n",
    "----------------\n",
    "                \n",
    "----------------\n",
    "Question: {question}\n",
    "----------------\n",
    "\n",
    "You must answer in Japanese.\n",
    "\n",
    "Helpful answer in Japanese:\"\"\" + ans\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=template, \n",
    "        input_variables=[\"ground_text\", \"question\"]\n",
    "    )\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm_qa, verbose=True)\n",
    "    answer = llm_chain.predict(ground_text=doc, question=q)\n",
    "     \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5277de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_doc = \"\"\n",
    "latest_q = None\n",
    "latest_ans = None\n",
    "\n",
    "# ボタンがクリックされたときに実行される関数\n",
    "def execute_func2(history):\n",
    "    global latest_ans, latest_doc, latest_q  # グローバル変数を参照\n",
    "\n",
    "    if latest_ans is not None:\n",
    "        answer = regenerate_qa(latest_doc, latest_q, latest_ans)\n",
    "        # func2の結果をUI上で表示\n",
    "        history = history + [(\"\", answer)]\n",
    "        return history\n",
    "\n",
    "# Chat UIの起動\n",
    "def add_text(history, text):\n",
    "    history = history + [(text, None)]\n",
    "    return history, \"\"\n",
    "\n",
    "def bot(history):\n",
    "    global latest_ans, latest_doc, latest_q  # グローバル変数を参照\n",
    "\n",
    "    latest_q = history[-1][0]\n",
    "    \n",
    "    retriever = index.as_retriever()\n",
    "    docs = retriever.get_relevant_documents(query=latest_q)\n",
    "    # print(f\"\\n参考箇所: \\n {docs}\")\n",
    "    latest_doc = \"\"\n",
    "    for d in docs:\n",
    "        latest_doc += d.page_content\n",
    "        latest_doc += \"\\n\"\n",
    "\n",
    "    latest_ans = generate_qa(latest_doc, latest_q)\n",
    "    print(f\"\\n回答: \\n {latest_ans}\")\n",
    "\n",
    "    retriever = index.as_retriever()\n",
    "    docs = retriever.get_relevant_documents(query=latest_q)\n",
    "    print(f\"\\n参考箇所: \\n {docs}\")\n",
    "    \n",
    "    history[-1][1] = latest_ans\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c34f1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1z/n4cgf80j1y5d5vlv2ws02_q40000gn/T/ipykernel_70356/3548103617.py:2: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "  chatbot = gr.Chatbot([], elem_id=\"chat with PDF\").style(height=600)\n",
      "/var/folders/1z/n4cgf80j1y5d5vlv2ws02_q40000gn/T/ipykernel_70356/3548103617.py:8: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "  ).style(container=False)\n",
      "/var/folders/1z/n4cgf80j1y5d5vlv2ws02_q40000gn/T/ipykernel_70356/3548103617.py:11: GradioUnusedKwargWarning: You have unused kwarg parameters in Button, please remove them: {'text': 'Execute Func2'}\n",
      "  btn = gr.Button(text=\"Execute Func2\", elem_id=\"execute_button\").style(height=20, width=120)\n",
      "/var/folders/1z/n4cgf80j1y5d5vlv2ws02_q40000gn/T/ipykernel_70356/3548103617.py:11: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.\n",
      "  btn = gr.Button(text=\"Execute Func2\", elem_id=\"execute_button\").style(height=20, width=120)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/makyua452144/anaconda3/envs/watsonx_demo/lib/python3.11/site-packages/gradio/analytics.py:46: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if StrictVersion(latest_pkg_version) > StrictVersion(current_pkg_version):\n",
      "/Users/makyua452144/anaconda3/envs/watsonx_demo/lib/python3.11/site-packages/gradio/analytics.py:46: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if StrictVersion(latest_pkg_version) > StrictVersion(current_pkg_version):\n",
      "/Users/makyua452144/anaconda3/envs/watsonx_demo/lib/python3.11/site-packages/chromadb/utils/read_write_lock.py:29: DeprecationWarning: notifyAll() is deprecated, use notify_all() instead\n",
      "  self._read_ready.notifyAll()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "----------------\n",
      "ます。\n",
      "\n",
      "ドロップダウンメニューから選択するだけで新しいモデルをお試しいただけます。スタジオの詳\n",
      "\n",
      "細については、こちらをご覧ください。\n",
      "\n",
      "未来への展望\n",
      "\n",
      "基盤モデルは、AIの展望を変えつつあり、進歩はさらに加速しています。私たちIBMは、この急\n",
      "\n",
      "速に進化する分野の最前線でイノベーションを真の企業価値に変えることをご支援していきま\n",
      "\n",
      "す。\n",
      "\n",
      "以上\n",
      "\n",
      "当報道資料は、2023年5月9日（現地時間）にIBM Corporationが発表したブログの抄訳です。原\n",
      "\n",
      "文はこちらをご参照ください。\n",
      "\n",
      "Release Categories\n",
      "\n",
      "AI\n",
      "\n",
      "United States — English\n",
      "\n",
      "Products & Solutions\n",
      "\n",
      "Learn about\n",
      "\n",
      "About IBM\n",
      "\n",
      "Top products & platforms\n",
      "\n",
      "What is Hybrid Cloud?\n",
      "\n",
      "Annual report\n",
      "\n",
      "Industries\n",
      "\n",
      "What is Artificial intelligence?\n",
      "\n",
      "Career opportunities\n",
      "\n",
      "Artificial intelligence\n",
      "\n",
      "What is Cloud Computing?\n",
      "\n",
      "Corporate social responsibility\n",
      "\n",
      "Blockchain\n",
      "\n",
      "What is Kubernetes?\n",
      "\n",
      "Diversity & inclusion\n",
      "\n",
      "Business operations\n",
      "\n",
      "What are Containers?\n",
      "\n",
      "Industry analyst reports\n",
      "Cloud computing\n",
      "\n",
      "What is DevOps?\n",
      "\n",
      "Investor relations\n",
      "\n",
      "Data & Analytics\n",
      "\n",
      "What is Machine Learning?\n",
      "\n",
      "News & announcements\n",
      "\n",
      "Hybrid cloud\n",
      "\n",
      "Thought leadership\n",
      "\n",
      "IT infrastructure\n",
      "\n",
      "Security, privacy & trust\n",
      "\n",
      "Popular links\n",
      "\n",
      "Security\n",
      "\n",
      "About IBM\n",
      "\n",
      "IBM Consulting\n",
      "\n",
      "Supply chain\n",
      "\n",
      "Communities\n",
      "\n",
      "Financing\n",
      "\n",
      "Developer education\n",
      "\n",
      "Follow IBM\n",
      "\n",
      "Support - Download fixes, updates & drivers\n",
      "\n",
      "LinkedIn\n",
      "\n",
      "Twitter\n",
      "\n",
      "IBM Research\n",
      "\n",
      "Instagram\n",
      "\n",
      "Partner with us - Partner Plus\n",
      "\n",
      "Subscription Center\n",
      "\n",
      "Training - Courses\n",
      "\n",
      "Upcoming events & webinars\n",
      "\n",
      "Cookieの設定\n",
      "\n",
      "Terms of use\n",
      "\n",
      "Contact IBM\n",
      "\n",
      "Accessibility\n",
      "\n",
      "Privacy\n",
      "いているかを物語るように、いくつかのツールはほんの数週間前に利用可能になったもので、こ\n",
      "\n",
      "の記事を書いている間にも新しいものが追加されています。\n",
      "\n",
      "今回発表のIBM watsonxの製品群のひとつであるwatsonx.aiには様々な機能が含まれ、今後も進\n",
      "\n",
      "化し続けますが、私たちの最も重要なお約束は、企業向けの安全な自動化製品を提供するという\n",
      "\n",
      "ことです。\n",
      "\n",
      "これは、AIにおける新しいパラダイムから価値を引き出すお客様の歩みを加速させるためのIBM\n",
      "\n",
      "の役割の一部です。ここでは、データおよびモデル・アーキテクチャーへのアプローチを含め、\n",
      "\n",
      "企業で活用できる、IBMで学習済みの一連の基盤モデルを構築してきた作業について説明しま\n",
      "\n",
      "す。また、私たちの新しいプラットフォームとツールについても紹介します。このプラットフォ\n",
      "\n",
      "ームでは、IBMが開発したモデルに加え、オープンソースの幅広いモデル・カタログを使用し\n",
      "\n",
      "て、企業が基盤モデルを活用したソリューションを構築・展開することができます。\n",
      "\n",
      "基盤モデルの土台となるデータ\n",
      "\n",
      "データの品質は重要です。バイアスを含んだデータや有害なデータで学習されたAIモデルは、当\n",
      "\n",
      "然、バイアスを含んだ回答や有害なアウトプットを生成する傾向があります。この問題は、基盤\n",
      "\n",
      "モデルの時代になるとさらに悪化します。基盤モデルでは、モデルの学習に使用するデータは通\n",
      "\n",
      "常、多くのソースから取得され、人間がすべてを十分精査することができないほど大量になるか\n",
      "\n",
      "らです。\n",
      "\n",
      "データは基盤モデルを動かす燃料であるため、IBMは、モデルに入るすべてのものを綿密にキュ\n",
      "まなタスクにおいて高い推論効率と性能レベルを提供します。Slate は、エンコーダーのみの\n",
      "\n",
      "RoBERTa ベースのモデル群を指し、生成型ではありませんが、多くの企業向けNLPタスクに高速\n",
      "\n",
      "かつ効果的に対応します。watsonx.aiのモデルはすべて、当社のカスタム設計のクラウド・ネイ\n",
      "\n",
      "ティブAIスーパーコンピュータ「Vela」上で、IBMが管理する企業向けのデータレイクを用いて\n",
      "\n",
      "学習されています。\n",
      "\n",
      "効率性とサステナビリティーは、watsonx.aiの中核となる設計の原則です。IBM Researchでは、\n",
      "\n",
      "小さなモデルを再利用して大きなモデルに「成長」させる「 LiGO」アルゴリズムなど、効率的な\n",
      "\n",
      "モデル学習のための新しい技術を開発してきました。この方法によって、モデルの学習に必要な\n",
      "\n",
      "時間、コスト、二酸化炭素排出量を40%〜70%削減することができます。推論速度を向上させる\n",
      "\n",
      "ために、私たちは量子化に関する専門知識を活用し、モデルを 32 ビット浮動小数点フォーマッ\n",
      "\n",
      "トよりはるかに小さい整数ビットフォーマットに縮小しています。AIモデルの精度を下げること\n",
      "\n",
      "で、精度を犠牲にすることなく効率性が大幅に向上します。この圧縮したモデルを、AIに最適化\n",
      "\n",
      "されたチップである「IBM AIU」でまもなく実行できるようにしたいと考えています。\n",
      "\n",
      "基盤モデル向けハイブリッドクラウド・ツール\n",
      "\n",
      "基盤モデルには、モデルのチューニングとデプロイのための使いやすいソフトウェア・プラット\n",
      "\n",
      "----------------\n",
      "                \n",
      "----------------\n",
      "Question: これはどのような製品ですか？\n",
      "----------------\n",
      "\n",
      "You must answer in Japanese.\n",
      "\n",
      "Helpful answer in Japanese:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "回答: \n",
      "  これは、IBM Watson Xの製品です。\n",
      "\n",
      "参考箇所: \n",
      " [Document(page_content='ます。\\n\\nドロップダウンメニューから選択するだけで新しいモデルをお試しいただけます。スタジオの詳\\n\\n細については、こちらをご覧ください。\\n\\n未来への展望\\n\\n基盤モデルは、AIの展望を変えつつあり、進歩はさらに加速しています。私たちIBMは、この急\\n\\n速に進化する分野の最前線でイノベーションを真の企業価値に変えることをご支援していきま\\n\\nす。\\n\\n以上\\n\\n当報道資料は、2023年5月9日（現地時間）にIBM Corporationが発表したブログの抄訳です。原\\n\\n文はこちらをご参照ください。\\n\\nRelease Categories\\n\\nAI\\n\\nUnited States — English\\n\\nProducts & Solutions\\n\\nLearn about\\n\\nAbout IBM\\n\\nTop products & platforms\\n\\nWhat is Hybrid Cloud?\\n\\nAnnual report\\n\\nIndustries\\n\\nWhat is Artificial intelligence?\\n\\nCareer opportunities\\n\\nArtificial intelligence\\n\\nWhat is Cloud Computing?\\n\\nCorporate social responsibility\\n\\nBlockchain\\n\\nWhat is Kubernetes?\\n\\nDiversity & inclusion\\n\\nBusiness operations\\n\\nWhat are Containers?\\n\\nIndustry analyst reports', metadata={}), Document(page_content='Cloud computing\\n\\nWhat is DevOps?\\n\\nInvestor relations\\n\\nData & Analytics\\n\\nWhat is Machine Learning?\\n\\nNews & announcements\\n\\nHybrid cloud\\n\\nThought leadership\\n\\nIT infrastructure\\n\\nSecurity, privacy & trust\\n\\nPopular links\\n\\nSecurity\\n\\nAbout IBM\\n\\nIBM Consulting\\n\\nSupply chain\\n\\nCommunities\\n\\nFinancing\\n\\nDeveloper education\\n\\nFollow IBM\\n\\nSupport - Download fixes, updates & drivers\\n\\nLinkedIn\\n\\nTwitter\\n\\nIBM Research\\n\\nInstagram\\n\\nPartner with us - Partner Plus\\n\\nSubscription Center\\n\\nTraining - Courses\\n\\nUpcoming events & webinars\\n\\nCookieの設定\\n\\nTerms of use\\n\\nContact IBM\\n\\nAccessibility\\n\\nPrivacy', metadata={}), Document(page_content='いているかを物語るように、いくつかのツールはほんの数週間前に利用可能になったもので、こ\\n\\nの記事を書いている間にも新しいものが追加されています。\\n\\n今回発表のIBM watsonxの製品群のひとつであるwatsonx.aiには様々な機能が含まれ、今後も進\\n\\n化し続けますが、私たちの最も重要なお約束は、企業向けの安全な自動化製品を提供するという\\n\\nことです。\\n\\nこれは、AIにおける新しいパラダイムから価値を引き出すお客様の歩みを加速させるためのIBM\\n\\nの役割の一部です。ここでは、データおよびモデル・アーキテクチャーへのアプローチを含め、\\n\\n企業で活用できる、IBMで学習済みの一連の基盤モデルを構築してきた作業について説明しま\\n\\nす。また、私たちの新しいプラットフォームとツールについても紹介します。このプラットフォ\\n\\nームでは、IBMが開発したモデルに加え、オープンソースの幅広いモデル・カタログを使用し\\n\\nて、企業が基盤モデルを活用したソリューションを構築・展開することができます。\\n\\n基盤モデルの土台となるデータ\\n\\nデータの品質は重要です。バイアスを含んだデータや有害なデータで学習されたAIモデルは、当\\n\\n然、バイアスを含んだ回答や有害なアウトプットを生成する傾向があります。この問題は、基盤\\n\\nモデルの時代になるとさらに悪化します。基盤モデルでは、モデルの学習に使用するデータは通\\n\\n常、多くのソースから取得され、人間がすべてを十分精査することができないほど大量になるか\\n\\nらです。\\n\\nデータは基盤モデルを動かす燃料であるため、IBMは、モデルに入るすべてのものを綿密にキュ', metadata={}), Document(page_content='まなタスクにおいて高い推論効率と性能レベルを提供します。Slate は、エンコーダーのみの\\n\\nRoBERTa ベースのモデル群を指し、生成型ではありませんが、多くの企業向けNLPタスクに高速\\n\\nかつ効果的に対応します。watsonx.aiのモデルはすべて、当社のカスタム設計のクラウド・ネイ\\n\\nティブAIスーパーコンピュータ「Vela」上で、IBMが管理する企業向けのデータレイクを用いて\\n\\n学習されています。\\n\\n効率性とサステナビリティーは、watsonx.aiの中核となる設計の原則です。IBM Researchでは、\\n\\n小さなモデルを再利用して大きなモデルに「成長」させる「 LiGO」アルゴリズムなど、効率的な\\n\\nモデル学習のための新しい技術を開発してきました。この方法によって、モデルの学習に必要な\\n\\n時間、コスト、二酸化炭素排出量を40%〜70%削減することができます。推論速度を向上させる\\n\\nために、私たちは量子化に関する専門知識を活用し、モデルを 32 ビット浮動小数点フォーマッ\\n\\nトよりはるかに小さい整数ビットフォーマットに縮小しています。AIモデルの精度を下げること\\n\\nで、精度を犠牲にすることなく効率性が大幅に向上します。この圧縮したモデルを、AIに最適化\\n\\nされたチップである「IBM AIU」でまもなく実行できるようにしたいと考えています。\\n\\n基盤モデル向けハイブリッドクラウド・ツール\\n\\n基盤モデルには、モデルのチューニングとデプロイのための使いやすいソフトウェア・プラット', metadata={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/makyua452144/anaconda3/envs/watsonx_demo/lib/python3.11/site-packages/chromadb/utils/read_write_lock.py:29: DeprecationWarning: notifyAll() is deprecated, use notify_all() instead\n",
      "  self._read_ready.notifyAll()\n",
      "/Users/makyua452144/anaconda3/envs/watsonx_demo/lib/python3.11/site-packages/chromadb/utils/read_write_lock.py:29: DeprecationWarning: notifyAll() is deprecated, use notify_all() instead\n",
      "  self._read_ready.notifyAll()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "----------------\n",
      "ます。\n",
      "\n",
      "ドロップダウンメニューから選択するだけで新しいモデルをお試しいただけます。スタジオの詳\n",
      "\n",
      "細については、こちらをご覧ください。\n",
      "\n",
      "未来への展望\n",
      "\n",
      "基盤モデルは、AIの展望を変えつつあり、進歩はさらに加速しています。私たちIBMは、この急\n",
      "\n",
      "速に進化する分野の最前線でイノベーションを真の企業価値に変えることをご支援していきま\n",
      "\n",
      "す。\n",
      "\n",
      "以上\n",
      "\n",
      "当報道資料は、2023年5月9日（現地時間）にIBM Corporationが発表したブログの抄訳です。原\n",
      "\n",
      "文はこちらをご参照ください。\n",
      "\n",
      "Release Categories\n",
      "\n",
      "AI\n",
      "\n",
      "United States — English\n",
      "\n",
      "Products & Solutions\n",
      "\n",
      "Learn about\n",
      "\n",
      "About IBM\n",
      "\n",
      "Top products & platforms\n",
      "\n",
      "What is Hybrid Cloud?\n",
      "\n",
      "Annual report\n",
      "\n",
      "Industries\n",
      "\n",
      "What is Artificial intelligence?\n",
      "\n",
      "Career opportunities\n",
      "\n",
      "Artificial intelligence\n",
      "\n",
      "What is Cloud Computing?\n",
      "\n",
      "Corporate social responsibility\n",
      "\n",
      "Blockchain\n",
      "\n",
      "What is Kubernetes?\n",
      "\n",
      "Diversity & inclusion\n",
      "\n",
      "Business operations\n",
      "\n",
      "What are Containers?\n",
      "\n",
      "Industry analyst reports\n",
      "まなタスクにおいて高い推論効率と性能レベルを提供します。Slate は、エンコーダーのみの\n",
      "\n",
      "RoBERTa ベースのモデル群を指し、生成型ではありませんが、多くの企業向けNLPタスクに高速\n",
      "\n",
      "かつ効果的に対応します。watsonx.aiのモデルはすべて、当社のカスタム設計のクラウド・ネイ\n",
      "\n",
      "ティブAIスーパーコンピュータ「Vela」上で、IBMが管理する企業向けのデータレイクを用いて\n",
      "\n",
      "学習されています。\n",
      "\n",
      "効率性とサステナビリティーは、watsonx.aiの中核となる設計の原則です。IBM Researchでは、\n",
      "\n",
      "小さなモデルを再利用して大きなモデルに「成長」させる「 LiGO」アルゴリズムなど、効率的な\n",
      "\n",
      "モデル学習のための新しい技術を開発してきました。この方法によって、モデルの学習に必要な\n",
      "\n",
      "時間、コスト、二酸化炭素排出量を40%〜70%削減することができます。推論速度を向上させる\n",
      "\n",
      "ために、私たちは量子化に関する専門知識を活用し、モデルを 32 ビット浮動小数点フォーマッ\n",
      "\n",
      "トよりはるかに小さい整数ビットフォーマットに縮小しています。AIモデルの精度を下げること\n",
      "\n",
      "で、精度を犠牲にすることなく効率性が大幅に向上します。この圧縮したモデルを、AIに最適化\n",
      "\n",
      "されたチップである「IBM AIU」でまもなく実行できるようにしたいと考えています。\n",
      "\n",
      "基盤モデル向けハイブリッドクラウド・ツール\n",
      "\n",
      "基盤モデルには、モデルのチューニングとデプロイのための使いやすいソフトウェア・プラット\n",
      "いているかを物語るように、いくつかのツールはほんの数週間前に利用可能になったもので、こ\n",
      "\n",
      "の記事を書いている間にも新しいものが追加されています。\n",
      "\n",
      "今回発表のIBM watsonxの製品群のひとつであるwatsonx.aiには様々な機能が含まれ、今後も進\n",
      "\n",
      "化し続けますが、私たちの最も重要なお約束は、企業向けの安全な自動化製品を提供するという\n",
      "\n",
      "ことです。\n",
      "\n",
      "これは、AIにおける新しいパラダイムから価値を引き出すお客様の歩みを加速させるためのIBM\n",
      "\n",
      "の役割の一部です。ここでは、データおよびモデル・アーキテクチャーへのアプローチを含め、\n",
      "\n",
      "企業で活用できる、IBMで学習済みの一連の基盤モデルを構築してきた作業について説明しま\n",
      "\n",
      "す。また、私たちの新しいプラットフォームとツールについても紹介します。このプラットフォ\n",
      "\n",
      "ームでは、IBMが開発したモデルに加え、オープンソースの幅広いモデル・カタログを使用し\n",
      "\n",
      "て、企業が基盤モデルを活用したソリューションを構築・展開することができます。\n",
      "\n",
      "基盤モデルの土台となるデータ\n",
      "\n",
      "データの品質は重要です。バイアスを含んだデータや有害なデータで学習されたAIモデルは、当\n",
      "\n",
      "然、バイアスを含んだ回答や有害なアウトプットを生成する傾向があります。この問題は、基盤\n",
      "\n",
      "モデルの時代になるとさらに悪化します。基盤モデルでは、モデルの学習に使用するデータは通\n",
      "\n",
      "常、多くのソースから取得され、人間がすべてを十分精査することができないほど大量になるか\n",
      "\n",
      "らです。\n",
      "\n",
      "データは基盤モデルを動かす燃料であるため、IBMは、モデルに入るすべてのものを綿密にキュ\n",
      "あります。その結果、企業はデータが豊富でビジネス価値が高いタスクの自動化に主眼を置き、\n",
      "\n",
      "それ以外は棚上げしてきました。しかし、この状況は変わり始めています。\n",
      "\n",
      "トランスフォーマーと自己教師あり学習法の登場により、膨大な量のラベルなしデータを利用で\n",
      "\n",
      "きるようになり、\"基盤モデル \"と呼ばれる大規模な事前学習済みモデルへの道が開かれました。\n",
      "\n",
      "これらの大規模なモデルは、自動化に関わるコストと労力を低減させました。\n",
      "\n",
      "基盤モデルは、様々なAIアプリケーションのための強力かつ汎用的な基盤を提供します。基盤モ\n",
      "\n",
      "デルを使用すると、限られた注釈付きデータと最小限の労力でタスクを迅速に実行でき、場合に\n",
      "\n",
      "よっては、ある課題を説明するだけで、モデルにその課題を解決させることができます。\n",
      "\n",
      "しかし、これらの強力なテクノロジーは、企業にとって新たなリスクや課題ももたらします。今\n",
      "\n",
      "日のモデルの多くは、品質や出所が不明なデータセットで学習しているため、攻撃的、あるいは\n",
      "\n",
      "バイアスを含んだり、事実と異なったことを回答する場合があります。また、大規模なモデル\n",
      "\n",
      "は、学習と実行にかかるコストが高く、エネルギーを大量に消費し、導入も複雑です。\n",
      "\n",
      "私たちIBMは、エンタープライズ向けの基盤モデルを利用するための本質的な課題を解決するア\n",
      "\n",
      "プローチを開発してきました。IBMは本日、市場にある最新のAIツールやテクノロジーにアクセ\n",
      "\n",
      "スするための入り口としてIBMが提供するwatsonx.aiを発表しました。この分野がいかに速く動\n",
      "\n",
      "----------------\n",
      "                \n",
      "----------------\n",
      "Question: この製品のビジネス上のメリットを教えて下さい。\n",
      "----------------\n",
      "\n",
      "You must answer in Japanese.\n",
      "\n",
      "Helpful answer in Japanese:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "回答: \n",
      " \n",
      "\n",
      "この製品のビジネス上のメリットは、以下の通りです。\n",
      "\n",
      "1. 効率性: WatsonXの基盤モデルは、学習に必要な時間、コスト、二酸化炭素排出量を40%〜70%削減することができます。\n",
      "2. 精度: この製品は、精度を犠牲にすることなく効率性を大幅に向上させることができます。\n",
      "3. 安全性: IBMは、データの品質を綿密にキューし、モデルに入るすべてのものを綿密に確認しています。\n",
      "4. 柔軟性: この製品は、様々なAIアプリケーションのための強力かつ汎用的な基盤を提供します。\n",
      "5. 新しいパラダイム: この製品は、新しいパラダイムから価値を引き出すお客様の歩みを加速させるためのIBMの役割の一部です。\n",
      "6. 統合性: この製品は、IBMが開発したモデルに加え、オープンソースの幅広いモデル・カタログを使用して、企業が基盤モデルを活用したソリューションを構築・展開することができます。\n",
      "\n",
      "参考箇所: \n",
      " [Document(page_content='ます。\\n\\nドロップダウンメニューから選択するだけで新しいモデルをお試しいただけます。スタジオの詳\\n\\n細については、こちらをご覧ください。\\n\\n未来への展望\\n\\n基盤モデルは、AIの展望を変えつつあり、進歩はさらに加速しています。私たちIBMは、この急\\n\\n速に進化する分野の最前線でイノベーションを真の企業価値に変えることをご支援していきま\\n\\nす。\\n\\n以上\\n\\n当報道資料は、2023年5月9日（現地時間）にIBM Corporationが発表したブログの抄訳です。原\\n\\n文はこちらをご参照ください。\\n\\nRelease Categories\\n\\nAI\\n\\nUnited States — English\\n\\nProducts & Solutions\\n\\nLearn about\\n\\nAbout IBM\\n\\nTop products & platforms\\n\\nWhat is Hybrid Cloud?\\n\\nAnnual report\\n\\nIndustries\\n\\nWhat is Artificial intelligence?\\n\\nCareer opportunities\\n\\nArtificial intelligence\\n\\nWhat is Cloud Computing?\\n\\nCorporate social responsibility\\n\\nBlockchain\\n\\nWhat is Kubernetes?\\n\\nDiversity & inclusion\\n\\nBusiness operations\\n\\nWhat are Containers?\\n\\nIndustry analyst reports', metadata={}), Document(page_content='まなタスクにおいて高い推論効率と性能レベルを提供します。Slate は、エンコーダーのみの\\n\\nRoBERTa ベースのモデル群を指し、生成型ではありませんが、多くの企業向けNLPタスクに高速\\n\\nかつ効果的に対応します。watsonx.aiのモデルはすべて、当社のカスタム設計のクラウド・ネイ\\n\\nティブAIスーパーコンピュータ「Vela」上で、IBMが管理する企業向けのデータレイクを用いて\\n\\n学習されています。\\n\\n効率性とサステナビリティーは、watsonx.aiの中核となる設計の原則です。IBM Researchでは、\\n\\n小さなモデルを再利用して大きなモデルに「成長」させる「 LiGO」アルゴリズムなど、効率的な\\n\\nモデル学習のための新しい技術を開発してきました。この方法によって、モデルの学習に必要な\\n\\n時間、コスト、二酸化炭素排出量を40%〜70%削減することができます。推論速度を向上させる\\n\\nために、私たちは量子化に関する専門知識を活用し、モデルを 32 ビット浮動小数点フォーマッ\\n\\nトよりはるかに小さい整数ビットフォーマットに縮小しています。AIモデルの精度を下げること\\n\\nで、精度を犠牲にすることなく効率性が大幅に向上します。この圧縮したモデルを、AIに最適化\\n\\nされたチップである「IBM AIU」でまもなく実行できるようにしたいと考えています。\\n\\n基盤モデル向けハイブリッドクラウド・ツール\\n\\n基盤モデルには、モデルのチューニングとデプロイのための使いやすいソフトウェア・プラット', metadata={}), Document(page_content='いているかを物語るように、いくつかのツールはほんの数週間前に利用可能になったもので、こ\\n\\nの記事を書いている間にも新しいものが追加されています。\\n\\n今回発表のIBM watsonxの製品群のひとつであるwatsonx.aiには様々な機能が含まれ、今後も進\\n\\n化し続けますが、私たちの最も重要なお約束は、企業向けの安全な自動化製品を提供するという\\n\\nことです。\\n\\nこれは、AIにおける新しいパラダイムから価値を引き出すお客様の歩みを加速させるためのIBM\\n\\nの役割の一部です。ここでは、データおよびモデル・アーキテクチャーへのアプローチを含め、\\n\\n企業で活用できる、IBMで学習済みの一連の基盤モデルを構築してきた作業について説明しま\\n\\nす。また、私たちの新しいプラットフォームとツールについても紹介します。このプラットフォ\\n\\nームでは、IBMが開発したモデルに加え、オープンソースの幅広いモデル・カタログを使用し\\n\\nて、企業が基盤モデルを活用したソリューションを構築・展開することができます。\\n\\n基盤モデルの土台となるデータ\\n\\nデータの品質は重要です。バイアスを含んだデータや有害なデータで学習されたAIモデルは、当\\n\\n然、バイアスを含んだ回答や有害なアウトプットを生成する傾向があります。この問題は、基盤\\n\\nモデルの時代になるとさらに悪化します。基盤モデルでは、モデルの学習に使用するデータは通\\n\\n常、多くのソースから取得され、人間がすべてを十分精査することができないほど大量になるか\\n\\nらです。\\n\\nデータは基盤モデルを動かす燃料であるため、IBMは、モデルに入るすべてのものを綿密にキュ', metadata={}), Document(page_content='あります。その結果、企業はデータが豊富でビジネス価値が高いタスクの自動化に主眼を置き、\\n\\nそれ以外は棚上げしてきました。しかし、この状況は変わり始めています。\\n\\nトランスフォーマーと自己教師あり学習法の登場により、膨大な量のラベルなしデータを利用で\\n\\nきるようになり、\"基盤モデル \"と呼ばれる大規模な事前学習済みモデルへの道が開かれました。\\n\\nこれらの大規模なモデルは、自動化に関わるコストと労力を低減させました。\\n\\n基盤モデルは、様々なAIアプリケーションのための強力かつ汎用的な基盤を提供します。基盤モ\\n\\nデルを使用すると、限られた注釈付きデータと最小限の労力でタスクを迅速に実行でき、場合に\\n\\nよっては、ある課題を説明するだけで、モデルにその課題を解決させることができます。\\n\\nしかし、これらの強力なテクノロジーは、企業にとって新たなリスクや課題ももたらします。今\\n\\n日のモデルの多くは、品質や出所が不明なデータセットで学習しているため、攻撃的、あるいは\\n\\nバイアスを含んだり、事実と異なったことを回答する場合があります。また、大規模なモデル\\n\\nは、学習と実行にかかるコストが高く、エネルギーを大量に消費し、導入も複雑です。\\n\\n私たちIBMは、エンタープライズ向けの基盤モデルを利用するための本質的な課題を解決するア\\n\\nプローチを開発してきました。IBMは本日、市場にある最新のAIツールやテクノロジーにアクセ\\n\\nスするための入り口としてIBMが提供するwatsonx.aiを発表しました。この分野がいかに速く動', metadata={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/makyua452144/anaconda3/envs/watsonx_demo/lib/python3.11/site-packages/chromadb/utils/read_write_lock.py:29: DeprecationWarning: notifyAll() is deprecated, use notify_all() instead\n",
      "  self._read_ready.notifyAll()\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot([], elem_id=\"chat with PDF\").style(height=600)\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=0.6):\n",
    "            txt = gr.Textbox(\n",
    "                show_label=False,\n",
    "                placeholder=\"Type your question and press enter\",\n",
    "            ).style(container=False)\n",
    "        with gr.Column(scale=0.4):\n",
    "            # ボタンを追加。ボタンがクリックされたときに`execute_func2`が呼ばれる。\n",
    "            btn = gr.Button(text=\"Execute Func2\", elem_id=\"execute_button\").style(height=20, width=120)\n",
    "            btn.click(execute_func2, inputs=chatbot, outputs=chatbot)\n",
    "            \n",
    "    txt.submit(add_text, [chatbot, txt], [chatbot, txt]).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee35d70",
   "metadata": {},
   "source": [
    "# グリッド検索"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b15b63ff",
   "metadata": {},
   "source": [
    "question = \"この製品のビジネス上のメリットを教えて下さい。\"\n",
    "\n",
    "rep = [1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]\n",
    "a = [0]\n",
    "b = [0]\n",
    "\n",
    "rab = [[rep[i], a[j], b[k]] for i in range(len(rep)) for j in range(len(a)) for k in range(len(b))]\n",
    "\n",
    "print(rab)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1855cb11",
   "metadata": {},
   "source": [
    "# 並列処理\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def jisaku_func(li, _index):\n",
    "    params_qa = GenerateParams(\n",
    "        decoding_method=\"greedy\",\n",
    "        min_new_tokens=min_new_tokens,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        stream=False,\n",
    "        repetition_penalty=li[0],\n",
    "    ).dict()  \n",
    "\n",
    "    llm_qa = LangChainInterface(model='meta-llama/llama-2-70b-chat', credentials=creds, params=params_qa)\n",
    "\n",
    "    #リトリーバーの設定(ユーザーから質問を受け取り、ベクターストアに投げかけ、関連コードを返してくれるところ)\n",
    "    retriever = index.as_retriever()\n",
    "    docs = retriever.get_relevant_documents(query=question)\n",
    "    # print(f\"\\n参考箇所: \\n {docs}\")\n",
    "    latest_doc = \"\"\n",
    "    for d in docs:\n",
    "        latest_doc += d.page_content\n",
    "        latest_doc += \"\\n\"\n",
    "\n",
    "    latest_ans = generate_qa(latest_doc, question)\n",
    "    print(f\"\\n回答: \\n {latest_ans}\")\n",
    "\n",
    "    retriever = index.as_retriever()\n",
    "    print(f\"\\n参考箇所: \\n {docs}\")\n",
    "    \n",
    "    \n",
    "    ans = latest_ans\n",
    "    print(li)\n",
    "        \n",
    "    return ans\n",
    "\n",
    "print('start\\n')\n",
    "start_time = time.time()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    res = list(executor.map(jisaku_func, rab, range(len(rab))))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'経過時間 {elapsed_time:.3f} 秒')\n",
    "print('\\nend')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25ff74d9",
   "metadata": {},
   "source": [
    "import openpyxl\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "d = now.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "import_file = \"res_\" + d + \".xlsx\"\n",
    "\n",
    "# 新しいワークブックを作成\n",
    "wb = openpyxl.Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "# 2次元リストのデータをExcelに書き込む\n",
    "for row_index, row in enumerate(rab, 1):  # 行のインデックスは1から始まる\n",
    "    for col_index, value in enumerate(row, 1):  # 列のインデックスも1から始まる\n",
    "        ws.cell(row=row_index, column=col_index, value=value)\n",
    "    ws.cell(row=row_index, column=4, value=res[row_index-1])\n",
    "\n",
    "# Excelファイルを保存\n",
    "wb.save(import_file)\n",
    "\n",
    "print(\"Excelファイルに書き込みが完了しました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7725e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:watsonx_demo] *",
   "language": "python",
   "name": "conda-env-watsonx_demo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
